## Hi, I'm Andrew Johnson ![](https://komarev.com/ghpvc/?username=andrew-johnson-4)

> $$ y^2 = x^3 + ax + b $$
>
> <div align="center">When a program has nothing surprising to say, it should say nothing. — Rule of Silence</div>
> <br>
> <div align="center">Mountains cannot be surmounted except by winding paths. — Johann Wolfgang von Goethe</div>


### [LSTS Examples & Documentation](https://lambda-mountain-compiler-backend.github.io/lsts-language-reference/)

### 2025 Roadmap (Working towards a verified kernel language)

<div>
  <p>
    <img align="left" height="100" src="https://github.com/andrew-johnson-4/andrew-johnson-4/blob/main/07723b62-1f56-4e7f-97d0-98423669a1da.png?raw=true"/>
    <span>If you are considering trying to learn LSTS, please wait until the V3 compiler and v1.0 language standard comes on line.
    v1.0 is the big goal for 2026.</span>
  </p>
  <br/>
  <br/>
  <br/>
</div>



* ✓ [1.22.0 Type-Directed Macros](https://github.com/Lambda-Mountain-Compiler-Backend/lambda-mountain/releases/tag/1.22.0)
* ✓ [1.23.0 Compiler 100% Written in LSTS](https://github.com/Lambda-Mountain-Compiler-Backend/lambda-mountain/releases/tag/1.23.0)
* ✓ [1.24.0 Garbage Collection](https://github.com/Lambda-Mountain-Compiler-Backend/lambda-mountain/releases/tag/1.24.0)
      
### Product Oriented Goals
* [probably.lm](https://github.com/andrew-johnson-4/probably.lm) Rational programming library
   * Simply put this allows neural nets and traditional code objects to integrate naturally
   * You don't need to understand math above boolean logic to program
   * Active learning if we can get performance good enough
   * Practically this gives infinitely more control over learning and inference with a neural model than a traditional model pipeline
   * Debugging models looks less like abstract geometry and more like traditional coded program flow
